{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19536,
     "status": "ok",
     "timestamp": 1731726752112,
     "user": {
      "displayName": "Felix Desjarlais",
      "userId": "09706425443483718445"
     },
     "user_tz": 480
    },
    "id": "pZ0ctR9OhO1-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import time\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from  sklearn.model_selection import KFold\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 164,
     "status": "ok",
     "timestamp": 1731726752130,
     "user": {
      "displayName": "Felix Desjarlais",
      "userId": "09706425443483718445"
     },
     "user_tz": 480
    },
    "id": "OHUPO9abhReg"
   },
   "outputs": [],
   "source": [
    "class SmokeDataset(object):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(self.root, \"images\"))))\n",
    "        self.annots = list(sorted(os.listdir(os.path.join(self.root, \"labels\"))))\n",
    "        self.classes = ['Smoke', 'Fire']\n",
    "\n",
    "    def convert_box_cord(self, bboxs, format_from, format_to, img_shape):\n",
    "        # only add valid bboxes\n",
    "        coords_converted = np.empty((0, 4))\n",
    "        for bb in bboxs:\n",
    "            if (bb[3] != 0 and bb[4] != 0):\n",
    "                coords_converted = np.append(coords_converted, [yolo2pixel([img_shape[1], img_shape[0]], bb[1:])], axis=0)\n",
    "\n",
    "        # if format_from == 'normxywh':\n",
    "        #     if format_to == 'xyminmax':\n",
    "        #         xw = bboxs[:, (1, 3)] * img_shape[1]\n",
    "        #         yh = bboxs[:, (2, 4)] * img_shape[0]\n",
    "        #         xmin = xw[:, 0] - xw[:, 1] / 2\n",
    "        #         xmax = xw[:, 0] + xw[:, 1] / 2\n",
    "        #         ymin = yh[:, 0] - yh[:, 1] / 2\n",
    "        #         ymax = yh[:, 0] + yh[:, 1] / 2\n",
    "        #         coords_converted = np.column_stack((xmin, ymin, xmax, ymax))\n",
    "\n",
    "        # print(\"cc: \", coords_converted)\n",
    "\n",
    "        return coords_converted\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and boxes\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
    "        annot_path = os.path.join(self.root, \"labels\", self.annots[idx])\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        img= img/255.0\n",
    "        labels = []#create an empty list for the labels\n",
    "        # retrieve bbox list and format to required type,\n",
    "        # if annotation file is empty, fill dummy box with label 0\n",
    "        if os.path.getsize(annot_path) != 0:\n",
    "            #read the labels line by line appending them to the labels list with +1 since 0 is reserved for background\n",
    "            # print(annot_path)\n",
    "            with open(annot_path, 'r') as f:\n",
    "              for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    # Only add index for valid bboxes\n",
    "                    if parts[3] != '0.0' and parts[4] != '0.0':\n",
    "                        # class_id = int(parts[0])\n",
    "                        class_id = int(parts[0])\n",
    "                        labels.append(class_id+1)\n",
    "            bboxs = np.loadtxt(annot_path, ndmin=2)\n",
    "            bboxs = self.convert_box_cord(bboxs, 'normxywh', 'xyminmax', img.shape)\n",
    "            num_objs = len(bboxs)\n",
    "            bboxs = torch.as_tensor(bboxs, dtype=torch.float32)\n",
    "            # there is only one class\n",
    "            # labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            # suppose all instances are not crowd\n",
    "            iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        else:\n",
    "            bboxs = torch.as_tensor([[0, 0, img.shape[1], img.shape[0]]], dtype=torch.float32)\n",
    "            labels = torch.zeros((1,), dtype=torch.int64)\n",
    "            iscrowd = torch.zeros((1,), dtype=torch.int64)\n",
    "\n",
    "        area = (bboxs[:, 3] - bboxs[:, 1]) * (bboxs[:, 2] - bboxs[:, 0])\n",
    "        image_id = torch.tensor([idx])\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = bboxs\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = idx\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            labels_np = labels.numpy().tolist()\n",
    "            # Pass bounding boxes and labels to Albumentations\n",
    "            sample = self.transforms(image=img, bboxes=target['boxes'], labels=labels_np)\n",
    "            img = sample['image']\n",
    "            target['boxes'] = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
    "            target['labels'] = torch.tensor(sample['labels'], dtype=torch.int64)\n",
    "\n",
    "        # Ensure boxes are valid after transformation\n",
    "        if target['boxes'].ndim == 1:\n",
    "            target['boxes'] = torch.as_tensor([[0, 0, 1, 1]], dtype=torch.float32)\n",
    "            target['labels'] = torch.zeros((1,), dtype=torch.int64)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1731726752134,
     "user": {
      "displayName": "Felix Desjarlais",
      "userId": "09706425443483718445"
     },
     "user_tz": 480
    },
    "id": "82x-rOFykbLJ"
   },
   "outputs": [],
   "source": [
    "def non_negative(coord, dim):\n",
    "        \"\"\"\n",
    "            Sets negative coordinates to zero. This fixes bugs in some labeling tools.\n",
    "\n",
    "            Input:\n",
    "                coord: Int or float\n",
    "                Any number that represents a coordinate, whether normalized or not.\n",
    "        \"\"\"\n",
    "\n",
    "        if coord < 0:\n",
    "            return 0\n",
    "        elif coord > dim:\n",
    "            return dim\n",
    "        else:\n",
    "            return coord\n",
    "\n",
    "def yolo2pixel(dim, yolo_coords):\n",
    "    \"\"\"\n",
    "        Transforms coordinates in YOLO format to coordinates in pixels.\n",
    "\n",
    "        Input:\n",
    "            dim: Tuple or list\n",
    "            Image size (width, height).\n",
    "            yolo_coords: List\n",
    "            Bounding box coordinates in YOLO format (xcenter, ycenter, width, height).\n",
    "        Output:\n",
    "            pixel_coords: List\n",
    "            Bounding box coordinates in pixels (xmin, ymin, xmax, ymax).\n",
    "    \"\"\"\n",
    "\n",
    "    xmin = non_negative(round(dim[0] * (yolo_coords[0] - yolo_coords[2]/2)), dim[0])\n",
    "    xmax = non_negative(round(dim[0] * (yolo_coords[0] + yolo_coords[2]/2)), dim[0])\n",
    "    ymin = non_negative(round(dim[1] * (yolo_coords[1] - yolo_coords[3]/2)), dim[1])\n",
    "    ymax = non_negative(round(dim[1] * (yolo_coords[1] + yolo_coords[3]/2)), dim[1])\n",
    "\n",
    "    pixel_coords = [xmin, ymin, xmax, ymax]\n",
    "\n",
    "    return pixel_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1731726752136,
     "user": {
      "displayName": "Felix Desjarlais",
      "userId": "09706425443483718445"
     },
     "user_tz": 480
    },
    "id": "lE0mLcFbhR9j"
   },
   "outputs": [],
   "source": [
    "def get_model_bbox(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    # default\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT') #pretrained=True)\n",
    "    \n",
    "    # v2\n",
    "    # model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights='DEFAULT') #pretrained=True)\n",
    "    \n",
    "    # mobilenet\n",
    "    # model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(weights='DEFAULT')\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # eff net ----\n",
    "    # backbone = torchvision.models.efficientnet_b0(weights='DEFAULT').features\n",
    "    # backbone.out_channels = 1280\n",
    "    \n",
    "    # anchor_generator = torchvision.models.detection.rpn.AnchorGenerator(\n",
    "    #     sizes=((32, 64, 128, 256, 512),),\n",
    "    #     aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    # )\n",
    "\n",
    "    # roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "    #     featmap_names=['0', '1', '2', '3'],\n",
    "    #     output_size=7,\n",
    "    #     sampling_ratio=2\n",
    "    # )\n",
    "\n",
    "    # model = torchvision.models.detection.FasterRCNN(\n",
    "    #     backbone=backbone,\n",
    "    #     num_classes=num_classes,\n",
    "    #     rpn_anchor_generator=anchor_generator,\n",
    "    #     box_roi_pool=roi_pooler\n",
    "    # )\n",
    "\n",
    "    # for layer in list(model.backbone.modules()):\n",
    "    #     for param in layer.parameters():\n",
    "    #         print(param.requires_grad)\n",
    "    #         param.requires_grad = True\n",
    "\n",
    "    # ----\n",
    "\n",
    "    # # unFreeze all parameters\n",
    "    # for layer in list(model.backbone.modules()):\n",
    "    #     for param in layer.parameters():\n",
    "    #         param.requires_grad = True\n",
    "\n",
    "    # # Freeze first 10 layers\n",
    "    # for layer in list(model.backbone.modules())[10:]:\n",
    "    #     for param in layer.parameters():\n",
    "    #         param.requires_grad = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1731726752143,
     "user": {
      "displayName": "Felix Desjarlais",
      "userId": "09706425443483718445"
     },
     "user_tz": 480
    },
    "id": "HKpGfHSKhUt8"
   },
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    if train:\n",
    "        return A.Compose([\n",
    "            A.Resize(height=600, width=600, p=1.0),\n",
    "            # A.Flip(p=0.5),\n",
    "            # A.RandomResizedCrop(height=640,width=640,p=0.4),\n",
    "            # # A.Perspective(p=0.4),\n",
    "            # A.Rotate(p=0.5),\n",
    "            # # A.Transpose(p=0.3),\n",
    "            ToTensorV2(p=1.0)],\n",
    "            bbox_params=A.BboxParams(format='pascal_voc',min_visibility=0.4, label_fields=['labels']))\n",
    "    else:\n",
    "        return A.Compose([A.Resize(height=600, width=600, p=1.0), ToTensorV2(p=1.0)],\n",
    "                         bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.5, label_fields=['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 121,
     "status": "ok",
     "timestamp": 1731726752148,
     "user": {
      "displayName": "Felix Desjarlais",
      "userId": "09706425443483718445"
     },
     "user_tz": 480
    },
    "id": "emyGlKPchWLI"
   },
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        print(f'Reset trainable parameters of layer = {layer}')\n",
    "        layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 958
    },
    "executionInfo": {
     "elapsed": 1751,
     "status": "ok",
     "timestamp": 1731726753789,
     "user": {
      "displayName": "Felix Desjarlais",
      "userId": "09706425443483718445"
     },
     "user_tz": 480
    },
    "id": "SDst3w5KhXnC",
    "outputId": "8f88cc53-8e6a-43af-da2b-79ec82ba4598"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import random\n",
    "\n",
    "# Function to visualize bounding boxes in the image with dynamic labeling\n",
    "def plot_img_bbox(img, target, class_names):\n",
    "    # plot the image and bboxes\n",
    "    fig, a = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(5, 5)\n",
    "    a.imshow(img.permute((1, 2, 0)))\n",
    "\n",
    "    # Loop over each bounding box in the target\n",
    "    for i, box in enumerate(target['boxes']):\n",
    "        x, y, width, height = box[0], box[1], box[2] - box[0], box[3] - box[1]\n",
    "        rect = patches.Rectangle((x, y),\n",
    "                                 width, height,\n",
    "                                 edgecolor='r',\n",
    "                                 facecolor='none',\n",
    "                                 clip_on=False)\n",
    "\n",
    "        # Retrieve the class name from `class_names` using the label index in `target['labels']`\n",
    "        label_idx = target['labels'][i].item()\n",
    "\n",
    "        label_name = class_names[label_idx-1]  # Get the label name based on the index\n",
    "\n",
    "        # Annotate the bounding box with the corresponding label name\n",
    "        a.annotate(label_name, (x, y - 10), color='red', weight='bold',\n",
    "                   fontsize=10, ha='left', va='top')\n",
    "\n",
    "        # Draw the bounding box on top of the image\n",
    "        a.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "# Assume `class_names` is a list of labels in the dataset\n",
    "dataset = SmokeDataset('DFire/train', get_transform(train=False))\n",
    "class_names = dataset.classes\n",
    "print(class_names)\n",
    "# Example of printing a few images with annotations\n",
    "for i in random.sample(range(len(dataset)), 3):\n",
    "    img, target = dataset[i]\n",
    "    print(target['labels'])\n",
    "    print(target['boxes'])\n",
    "    plot_img_bbox(img, target, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JiesOdHt2v6"
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1731726789354,
     "user": {
      "displayName": "Felix Desjarlais",
      "userId": "09706425443483718445"
     },
     "user_tz": 480
    },
    "id": "2WIXPMhmuhtl"
   },
   "outputs": [],
   "source": [
    "# !mkdir bestEpochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1731726790841,
     "user": {
      "displayName": "Felix Desjarlais",
      "userId": "09706425443483718445"
     },
     "user_tz": 480
    },
    "id": "Oik9KtnGYHAk"
   },
   "outputs": [],
   "source": [
    "save_dir = 'bestEpochs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "executionInfo": {
     "elapsed": 1450,
     "status": "error",
     "timestamp": 1731727076860,
     "user": {
      "displayName": "Felix Desjarlais",
      "userId": "09706425443483718445"
     },
     "user_tz": 480
    },
    "id": "1HnATb7at3EB",
    "outputId": "937801c6-ac87-4f16-b9d7-033aa21f2c8d"
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "# three classes: background, smoke, and fire\n",
    "num_classes = 3\n",
    "# use our dataset and defined transformations\n",
    "dataset = SmokeDataset('DFire/train', get_transform(train=True))\n",
    "dataset_val = SmokeDataset('DFire/test', get_transform(train=False))\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=5, shuffle=True, num_workers=0, # was 2, but anything greater than 0 doesn't work on windows\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=5, shuffle=False, num_workers=0, # was 2, but anything greater than 0 doesn't work on windows\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_bbox(num_classes)\n",
    "\n",
    "'''\n",
    "Use this to reset all trainable weights\n",
    "model.apply(reset_weights)\n",
    "'''\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0025,  # Feel free to play with values (0.005, 0.9, 0)\n",
    "                            momentum=0.9, weight_decay=0)\n",
    "\n",
    "# Defining learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=20,\n",
    "                                                gamma=0.2)\n",
    "\n",
    "\n",
    "result_mAP = []\n",
    "best_epoch = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 50 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    results =  evaluate(model, data_loader_val, device=device)\n",
    "    # saves results of mAP @ IoU = 0.5\n",
    "    result_mAP.append(results.coco_eval['bbox'].stats[1])\n",
    "    #save the best result so far\n",
    "    if result_mAP[-1] == max(result_mAP):\n",
    "        best_save_path = os.path.join(f'{save_dir}/smoke_bestmodel_noaug_sgd(wd=0)_8batch-epoch{epoch}.pth')\n",
    "        torch.save(model.state_dict(), best_save_path)\n",
    "        best_epoch = int(epoch)\n",
    "        print(f'model from epoch number {epoch} saved!\\n result is {max(result_mAP)}')\n",
    "\n",
    "# Saving the last model\n",
    "save_path = os.path.join(f'smoke_noaug_sgd_2batch-lastepoch{num_epochs-1}.pth')\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f'model from last epoch(no.{num_epochs-1}) saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F2gmPr7u8um"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = get_model_bbox(num_classes)\n",
    "model.load_state_dict(torch.load(os.path.join(f'bestEpochs/smoke_bestmodel_noaug_sgd(wd=0)_8batch-epoch{best_epoch}.pth'),map_location=device))\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGOlE6IVu-6_"
   },
   "outputs": [],
   "source": [
    "color_inference = np.array([0.0,0.0,255.0])\n",
    "color_label = np.array([255.0,0.0,0.0])\n",
    "\n",
    "detection_threshold = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "image_path = 'some_picture.jpg'\n",
    "# image_path = 'smoke/train/images/ck0khubxx5khq0794ja58vgv5_jpeg_jpg.rf.e9a3da4068323430011b1a2300c02074.jpg'\n",
    "\n",
    "model_image = cv2.imread(image_path)\n",
    "model_image = cv2.cvtColor(model_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "model_image = model_image/255.0\n",
    "\n",
    "transform = A.Compose([A.Resize(height=600, width=600, p=1.0), ToTensorV2()])\n",
    "model_image = transform(image=model_image)['image']\n",
    "\n",
    "cv2_image = np.transpose(model_image.numpy()*255,(1, 2, 0)).astype(np.float32)\n",
    "cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_RGB2BGR).astype(np.float32)\n",
    "\n",
    "# add batch dimension\n",
    "model_image = torch.unsqueeze(model_image, 0)\n",
    "with torch.no_grad():\n",
    "    outputs = model(model_image.to(device))\n",
    "    \n",
    "# load all detection to CPU for further operations\n",
    "outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
    "\n",
    "# print(outputs[0])\n",
    "label_names = [\"Background\", \"Smoke\", \"Fire\"]\n",
    "\n",
    "# carry further only if there are detected boxes\n",
    "if len(outputs[0]['boxes']) != 0:\n",
    "    boxes = outputs[0]['boxes'].data.numpy()\n",
    "    scores = outputs[0]['scores'].data.numpy()\n",
    "    # filter out boxes according to `detection_threshold`\n",
    "    boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "    scores = np.round(scores[scores >= detection_threshold],2)\n",
    "    draw_boxes = boxes.copy()\n",
    "\n",
    "\n",
    "    # draw the bounding boxes and write the class name on top of it\n",
    "    for j,box in enumerate(draw_boxes):\n",
    "        cv2.rectangle(cv2_image,\n",
    "                      (int(box[0]), int(box[1])),\n",
    "                      (int(box[2]), int(box[3])),\n",
    "                      color_inference, 2)\n",
    "        cv2.putText(img=cv2_image, text=label_names[outputs[0]['labels'][j]], #,\"Smoke\",\n",
    "                    org=(int(box[0] + 4), int(box[1] + 8)),\n",
    "                    fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_inference,\n",
    "                    thickness=1, lineType=cv2.LINE_AA)\n",
    "        cv2.putText(img=cv2_image, text=str(scores[j]),\n",
    "                    org=(int(box[2] - 24), int(box[1] + 8)),\n",
    "                    fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_inference,\n",
    "                    thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "    # set size\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # convert color from CV2 BGR back to RGB\n",
    "    plt_image = cv2.cvtColor(cv2_image/255.0, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(plt_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84974,
     "status": "ok",
     "timestamp": 1731710662855,
     "user": {
      "displayName": "Felix Desjarlais",
      "userId": "09706425443483718445"
     },
     "user_tz": 480
    },
    "id": "4J7DbJseu1KQ",
    "outputId": "644a9e73-f5b2-4bd1-eb82-6fef58d1328b"
   },
   "outputs": [],
   "source": [
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset_test = SmokeDataset('smoke/train', get_transform(train=False))\n",
    "\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=0, # was 2\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_bbox(num_classes)\n",
    "\n",
    "\n",
    "# load model to evaluate\n",
    "model.load_state_dict(torch.load(os.path.join(f'bestEpochs/smoke_bestmodel_noaug_sgd(wd=0)_8batch-epoch{best_epoch}.pth'),map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "results = evaluate(model, data_loader_test, device=device)\n",
    "print(\"Average Precision at IoU 0.50:\", results.coco_eval['bbox'].stats[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGOlE6IVu-6_"
   },
   "outputs": [],
   "source": [
    "color_inference = np.array([0.0,0.0,255.0])\n",
    "color_label = np.array([255.0,0.0,0.0])\n",
    "\n",
    "detection_threshold = 0.7 # 0.7 originally\n",
    "# to count the total number of images iterated through\n",
    "frame_count = 0\n",
    "# to keep adding the FPS for each image\n",
    "total_fps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 72617,
     "status": "ok",
     "timestamp": 1731710735465,
     "user": {
      "displayName": "Felix Desjarlais",
      "userId": "09706425443483718445"
     },
     "user_tz": 480
    },
    "id": "tJKb8vBivJ5b",
    "outputId": "81b4adc3-0697-4213-96a1-d0ba86f132c4"
   },
   "outputs": [],
   "source": [
    "for i,data in enumerate(data_loader_test):\n",
    "    # get the image file name for predictions file name\n",
    "    image_name = 'image no:' + str(int(data[1][0]['image_id']))\n",
    "    model_image = data[0][0]\n",
    "    cv2_image = np.transpose(model_image.numpy()*255,(1, 2, 0)).astype(np.float32)\n",
    "    cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_RGB2BGR).astype(np.float32)\n",
    "\n",
    "    # add batch dimension\n",
    "    model_image = torch.unsqueeze(model_image, 0)\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(model_image.to(device))\n",
    "    end_time = time.time()\n",
    "    # get the current fps\n",
    "    fps = 1 / (end_time - start_time)\n",
    "    # add `fps` to `total_fps`\n",
    "    total_fps += fps\n",
    "    # increment frame count\n",
    "    frame_count += 1\n",
    "    # load all detection to CPU for further operations\n",
    "    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
    "    # carry further only if there are detected boxes\n",
    "    if len(outputs[0]['boxes']) != 0:\n",
    "        boxes = outputs[0]['boxes'].data.numpy()\n",
    "        scores = outputs[0]['scores'].data.numpy()\n",
    "        # filter out boxes according to `detection_threshold`\n",
    "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "        scores = np.round(scores[scores >= detection_threshold],2)\n",
    "        draw_boxes = boxes.copy()\n",
    "\n",
    "\n",
    "        # draw the bounding boxes and write the class name on top of it\n",
    "        for j,box in enumerate(draw_boxes):\n",
    "            cv2.rectangle(cv2_image,\n",
    "                          (int(box[0]), int(box[1])),\n",
    "                          (int(box[2]), int(box[3])),\n",
    "                          color_inference, 2)\n",
    "            cv2.putText(img=cv2_image, text=\"Smoke\",\n",
    "                        org=(int(box[0]), int(box[1] - 5)),\n",
    "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_inference,\n",
    "                        thickness=1, lineType=cv2.LINE_AA)\n",
    "            cv2.putText(img=cv2_image, text=str(scores[j]),\n",
    "                        org=(int(box[0]), int(box[1] + 8)),\n",
    "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_inference,\n",
    "                        thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "        # add boxes for labels\n",
    "        for box in data[1][0]['boxes']:\n",
    "            cv2.rectangle(cv2_image,\n",
    "                          (int(box[0]), int(box[1])),\n",
    "                          (int(box[2]), int(box[3])),\n",
    "                          color_label, 2)\n",
    "            cv2.putText(img=cv2_image, text=\"Label\",\n",
    "                        org=(int(box[0]), int(box[1] - 5)),\n",
    "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_label,\n",
    "                        thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        # set size\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # convert color from CV2 BGR back to RGB\n",
    "        plt_image = cv2.cvtColor(cv2_image/255.0, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(plt_image)\n",
    "        plt.show()\n",
    "        cv2.imwrite(f\"/content/Results/{image_name}.jpg\", cv2_image)\n",
    "    print(f\"Image {i + 1} done...\")\n",
    "    print('-' * 50)\n",
    "print('TEST PREDICTIONS COMPLETE')\n",
    "\n",
    "avg_fps = total_fps / frame_count\n",
    "print(f\"Average FPS: {avg_fps:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
